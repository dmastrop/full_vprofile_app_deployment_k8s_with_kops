# Summary of project 14


Project 14. This is a full stack deployment of the vprofile java app ( frontend tomcat, memcached, rabbitmq and   backend mysql) onto a k8s cluster created with kops. This is meant to be a production grade roll out of the   **containerized** version of the vprofile java app onto k8s. Use EC2 instance on AWS2 with kubectl, aws cli,kops   
installed/IAM  


# Detail on the implementation:

## First get the k8s yaml files from the vprofile-project (branch kube-app branch)

## Getting kops up using the EC2 instance kops-project14-EC2

The EC2 instance has to have aws cli and kops installed as well as kubectl.  
ubuntu@ip-172-31-88-11:~$ kops version  
Client version: 1.26.4 (git-v1.26.4)  
ubuntu@ip-172-31-88-11:~$ aws --version  
aws-cli/1.22.34 Python/3.10.12 Linux/6.5.0-1016-aws botocore/1.23.34  
ubuntu@ip-172-31-88-11:~$ kubectl version --client  
Client Version: v1.29.3  
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3  

The EC2 instance has the kops ssh key 

## AWS access key and secret key are already set on the EC2 instance for the apppropriate k8s cluster permissions 

aws configure list will show the IAM user already configured.

## test the Route53 hosted zone that is designated for the k8s cluster creation with kops

dig NS kops-project14.xxxxx.com
nslookup -type=ns kops-project14.xxxxx.com

The NS should show the AWS name servers for the above.


## The EC2 instance is kops-project14-EC2

The SSH key for the instance is kops-project14-rsa-pem.pem
ssh -i kops-project14-rsa-pem.pem ubuntu@<EC2_instance_public_ip>

The EC2 instance has an rsa generated keypair (kops-key-project14 and .pub) to be used with the kops configuration and deployment so that
the control master and worker nodes can be accessed from EC2 to the cluster.


% ssh -i kops-project14-rsa-pem.pem ubuntu@44.204.236.93 

Last login: Fri Apr 26 02:44:55 2024 from 73.202.248.175
ubuntu@ip-172-31-88-11:~$ 

ubuntu@ip-172-31-88-11:~$ ls ~/.ssh
authorized_keys  known_hosts  known_hosts.old  kops-key-project14  kops-key-project14.pub


## Configure the cluster with kops

Note that the S3 bucket has to be preconfigured

kops create cluster --name=kops-project14.holinessinloveofchrist.com \
--state=s3://vprofile-kops-s3-state-project14 --zones=us-east-1a,us-east-1b \
--node-count=2 --node-size=t3.small --master-size=t3.medium --dns-zone=kops-project14.holinessinloveofchrist.com \
--node-volume-size=8 --master-volume-size=8 --ssh-public-key=~/.ssh/kops-key-project14.pub


## Deploy the cluster with the above configuration

kops update cluster --name=kops-project14.holinessinloveofchrist.com --state=s3://vprofile-kops-s3-state-project14 --yes --admin

## Validate the cluster (node state)

kops validate cluster --state=s3://vprofile-kops-s3-state-project14


ubuntu@ip-172-31-88-11:~$ kops validate cluster --state=s3://vprofile-kops-s3-state-project14
Using cluster from kubectl context: kops-project14.holinessinloveofchrist.com

Validating cluster kops-project14.holinessinloveofchrist.com

INSTANCE GROUPS
NAME                            ROLE            MACHINETYPE     MIN     MAX     SUBNETS
control-plane-us-east-1a        ControlPlane    t3.medium       1       1       us-east-1a
nodes-us-east-1a                Node            t3.small        1       1       us-east-1a
nodes-us-east-1b                Node            t3.small        1       1       us-east-1b

NODE STATUS
NAME                    ROLE            READY
i-002c39301f5dc9bb2     node            True
i-050ff1a6e0e0fe532     control-plane   True
i-0ec3eab1e5bb72adb     node            True


ubuntu@ip-172-31-88-11:~$ kubectl get node
NAME                  STATUS   ROLES           AGE     VERSION
i-002c39301f5dc9bb2   Ready    node            7m34s   v1.26.13
i-050ff1a6e0e0fe532   Ready    control-plane   10m     v1.26.13
i-0ec3eab1e5bb72adb   Ready    node            7m43s   v1.26.13


## optionally can ssh into the master control node if required (note clean up the ~/.ssh//known_hosts file with older keys may be required)

ubuntu@ip-172-31-88-11:~$ ssh -i ~/.ssh/kops-key-project14 ubuntu@api.kops-project14.holinessinloveofchrist.com
The authenticity of host 'api.kops-project14.holinessinloveofchrist.com (44.203.96.195)' can't be established.
ED25519 key fingerprint is SHA256:bpckAK8QhWOGNmd5l4+r396Ye5BpJpLSvmR6/7aIVBk.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added 'api.kops-project14.holinessinloveofchrist.com' (ED25519) to the list of known hosts.
Welcome to Ubuntu 20.04.6 LTS (GNU/Linux 5.15.0-1052-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

  System information as of Fri Apr 26 19:13:12 UTC 2024

  System load:  0.93              Processes:                    160
  Usage of /:   79.9% of 7.57GB   Users logged in:              0
  Memory usage: 34%               IPv4 address for cilium_host: 100.96.0.207
  Swap usage:   0%                IPv4 address for ens5:        172.20.48.241

Expanded Security Maintenance for Applications is not enabled.

60 updates can be applied immediately.
49 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable

Enable ESM Apps to receive additional future security updates.
See https://ubuntu.com/esm or run: sudo pro status



The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

To run a command as administrator (user "root"), use "sudo <command>".
See "man sudo_root" for details.

ubuntu@i-050ff1a6e0e0fe532:~$ 





## Once the cluster is up, the setup is ready to apply the k8s yaml configuration files for the full vprofile stack (application and backend server infrastructure)

To get the full stack up this project uses public docker images for memcached and rabbitmq
The database container is a customized docker image vprofile/vprofiledb:V1
Likewise the vprofileapp image is a customized docker image as well vprofile/vprofileapp:V1
This  image was created from the source code on the vprofile-project docker branch (must use the docker branch)
More on this in a section below.
The application.properties endpoint file access is configured using local k8s hostnames as the entire server infra is on k8s network fabric and is flat. So complex AWS endpoints are not required.
All the sensitive login information is in the appplication.properties file, but this will be built into the container and never exposed
The private network is flat and permits routing server to server by default.   This is a major deployment advantage of deploying containers to k8s cluster for orchestration rather than deploying with EC2 instances.  No explicit security groups need to be configured in this k8s deployment of the full stack, unlike with an EC2 instance deployment and no complex endpoint URLs need to be configured in the application.properties file.





## One prerequisite prior to applying the yaml files to the cluster is to configure an EBS (Elastic Block store) standard volume in the same region as the cluster (us-east-1) so that the mysql db containter can store /var/lib/mysql into the EBS volume. The db container itself has no provision for this external volume.  In addtion the volume must be in the same availablity zone as the node that has the db pod.  (See further below on how to ensure this)

This can be done from the VSCode terminal.
Make sure to configure the terminal with the AWS2 account (not the AWS1 account). Use the admin user IAMidadave
export AWS_PROFILE=IAMidadave
% echo $AWS_PROFILE
IAMidadave

aws configure list to make sure the correct user is used. This is an Admin user.
aws ec2 create-volume --availability-zone=us-east-1a  --region=us-east-1 --size=3 --volume-type=gp2

aws ec2 create-volume --availability-zone=us-east-1a  --region=us-east-1 --size=3 --volume-type=gp2
{
    "AvailabilityZone": "us-east-1a",  <<<< must be in this availablity zone for reasons below
    "CreateTime": "2024-04-26T19:39:43+00:00",
    "Encrypted": false,
    "Size": 3,
    "SnapshotId": "",
    "State": "creating",
    "VolumeId": "vol-01d2206ce52103df9",   <<< volume id
    "Iops": 100,
    "Tags": [],
    "VolumeType": "gp2",
    "MultiAttachEnabled": false
}
:


## NOTE: the pod running the mysql db must be running on a node in the SAME region AND same availability zone as the EBS created above. To do this use the following code in the mysql db deployment yaml  (the nodes must also be explicitly labeled with the availability zones; see next section below)

      nodeSelector:
        zone: us-east-1a
        # we created labels for the 2 nodes in the kops cluser: us-east-1a and us-east-1b
        # This nodeSelector will ensure that this db container runs on this us-east-1a node
        # this will ensure that the db container is in the same zone as the volume block store that it is using
        # The volume block store was manually created in us-east-1a



## This is how to label the  nodes in the cluster with the availability zone that they are in. This must be done for the reasons above, to correlate node availability zone to the EBS availability zone.

ubuntu@ip-172-31-88-11:~$ kubectl get nodes --show-labels
NAME                  STATUS   ROLES           AGE   VERSION    LABELS
i-002c39301f5dc9bb2   Ready    node            43m   v1.26.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t3.small,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1b,kubernetes.io/arch=amd64,kubernetes.io/hostname=i-002c39301f5dc9bb2,kubernetes.io/os=linux,node-role.kubernetes.io/node=,node.kubernetes.io/instance-type=t3.small,topology.ebs.csi.aws.com/zone=us-east-1b,topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1b

i-050ff1a6e0e0fe532   Ready    control-plane   46m   v1.26.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t3.medium,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1a,kops.k8s.io/kops-controller-pki=,kubernetes.io/arch=amd64,kubernetes.io/hostname=i-050ff1a6e0e0fe532,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=,node.kubernetes.io/instance-type=t3.medium,topology.ebs.csi.aws.com/zone=us-east-1a,topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1a

i-0ec3eab1e5bb72adb   Ready    node            43m   v1.26.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t3.small,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1a,kubernetes.io/arch=amd64,kubernetes.io/hostname=i-0ec3eab1e5bb72adb,kubernetes.io/os=linux,node-role.kubernetes.io/node=,node.kubernetes.io/instance-type=t3.small,topology.ebs.csi.aws.com/zone=us-east-1a,topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1a

the last node in the output above i-0ec3eab1e5bb72adb  is the worker node where the mysql db has to be run since that is in the same availability zone as the EBS store created above

ubuntu@ip-172-31-88-11:~$ kubectl describe node i-0ec3eab1e5bb72adb |grep us-east-1
                    failure-domain.beta.kubernetes.io/region=us-east-1
                    failure-domain.beta.kubernetes.io/zone=us-east-1a
                    topology.ebs.csi.aws.com/zone=us-east-1a
                    topology.kubernetes.io/region=us-east-1
                    topology.kubernetes.io/zone=us-east-1a
ProviderID:                   aws:///us-east-1a/i-0ec3eab1e5bb72adb




## label the nodes with availabilty zone


ubuntu@ip-172-31-88-11:~$ kubectl label nodes i-0ec3eab1e5bb72adb zone=us-east-1a
node/i-0ec3eab1e5bb72adb labeled
ubuntu@ip-172-31-88-11:~$ kubectl label nodes i-002c39301f5dc9bb2 zone=us-east-1b
node/i-002c39301f5dc9bb2 labeled


## the worker nodes are now labeled:

ubuntu@ip-172-31-88-11:~$ kubectl get nodes --show-labels
NAME                  STATUS   ROLES           AGE   VERSION    LABELS
i-002c39301f5dc9bb2   Ready    node            50m   v1.26.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t3.small,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1b,kubernetes.io/arch=amd64,kubernetes.io/hostname=i-002c39301f5dc9bb2,kubernetes.io/os=linux,node-role.kubernetes.io/node=,node.kubernetes.io/instance-type=t3.small,topology.ebs.csi.aws.com/zone=us-east-1b,topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1b,zone=us-east-1b  <<<<<<<<<

i-050ff1a6e0e0fe532   Ready    control-plane   53m   v1.26.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t3.medium,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1a,kops.k8s.io/kops-controller-pki=,kubernetes.io/arch=amd64,kubernetes.io/hostname=i-050ff1a6e0e0fe532,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=,node.kubernetes.io/instance-type=t3.medium,topology.ebs.csi.aws.com/zone=us-east-1a,topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1a


i-0ec3eab1e5bb72adb   Ready    node            50m   v1.26.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t3.small,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1a,kubernetes.io/arch=amd64,kubernetes.io/hostname=i-0ec3eab1e5bb72adb,kubernetes.io/os=linux,node-role.kubernetes.io/node=,node.kubernetes.io/instance-type=t3.small,topology.ebs.csi.aws.com/zone=us-east-1a,topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1a,zone=us-east-1a   <<<<<<<<<<


## at this point the EBS volume is in availablity zone us-east-1a and the nodes are labeled with avaialbity zone us-east-1a and the nodeSelector of the mysql db pod deployment yaml is set to us-east-1a.  




## yaml code (partial excerpt) for the volumeMount of the volume in the next section below. This is in the db deployment yaml file.

spec:
      containers:
        - name: vprodb
          image: vprofile/vprofiledb:V1
          # use the image on docker hub vprofile/vprofiledb:V1
            
          # USE an initContainer (bottom, below) instead for the removal of the lost+found directory on the volumne
          # args:
          #   - "--ignore-db-dir=lost+found"

          # map the volume defined at bottom below to this db container pod
          volumeMounts:
            - mountPath: /var/lib/mysql
            # the volume will be mounted to this directory path on the db container
              name: vpro-db-data
              # name is below

## yaml code for the database deployment yaml file. Insert this at the bottom of the file.  This volumeID has be be manually added, for now,  to the yaml after creating the volume on EBS.  Note it has to be tagged on AWS accordingly with cluster name.  The initContainer block is just to remove the lost+found folder on the default volume file structure. This will cause problems when the db container tries to do the mountVolume above. Removing it is the best approach.

     volumes:
      # attach this volume to this db container defined above
        - name: vpro-db-data
          awsElasticBlockStore:
            volumeID: vol-01d2206ce52103df9  <<<<<<<<<<<<<< update this with the new EBS volumne created above
            # the volume id is from AWS cli when we created the volume
            # this is vol-0b308063deec65e5e
            fsType: ext4
            # to attach to the volume in k8s we need to tag the AWS volume
            # We can do this through aws cli or through the web console.
            # the tag must use ClusterName as key and the name of the kops clustser as the value
            # Without the tag will get a permissions denied error
            # Map the volume to the pod above wiht the volumenMounts (see above)
            # Note with ext4 fsType there will be a lost+found diretory created by linux
            # this will be a problem when the db container tries to mount this volume and map it.
            # Must either ignore the folder or before mysql starts delete the folder
            # To ignore pass args tp the container.(see above)  This only works with mysql 5 not with 8
            # Better to do an init container that starts on the pod prior to the db container. This will remove the folder prior to the db container starting
            # on the pod.
      initContainers:
      # once the initContainer removes the folder below it will exit and the db container will come up and
      # mount the same volume, but with no lost+found folder
        - name: busybox
          # name can be anything
          image: busybox:latest
          args: ["rm", "-rf", "/var/lib/mysql/lost+found"]
          # this arg will remove the folder lost+found from the mounted directory 
          # we will mount the volume to this  lightweight container to perform the deletion
          volumeMounts:
            - name: vpro-db-data
              mountPath: /var/lib/mysql
              # the volume will be mounted to this directory path on the db container




## Tag the db EBS volume created above with the tag KubernetesCluster and the value of name of the kops cluster which is kops-project14.holinessinloveofchrist.com (the hosted zone name on Route53 and kops cluster name)

This is done through the AWS web console


## the vprofile/vprofileapp:V1 docker image used for the vprofile app frontend is an elastic loadbalancer rather than an nginx container.  The docker image was created using source in vprofile-project and using docker branch

The frontend app artifact .war file is embedded in the tomcat container instance as ROOT.war in the home directory of the tomcat server

FROM tomcat:8-jre11
LABEL "Project"="Vprofile"
LABEL "Author"="xxxxxxx"

RUN rm -rf /usr/local/tomcat/webapps/*
COPY target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

EXPOSE 8080
CMD ["catalina.sh", "run"]
WORKDIR /usr/local/tomcat/
VOLUME /usr/local/tomcat/webapps





## The application.properties file embedded in the built docker container image is below.  The sensitive information is **** out.  Note the "endpoints" are simply hostnames that will be configured in the corresponding SERVICE yaml files for each of the servers (pods) below.  This is a major advantage to deloying more complex full stack applications that involve multiple server and backend servers. Similarly the ports will be defined in the corresponding SERVICE yaml files for the applications/servers below.


Note the port numbers. These will be required for the yaml k8s definition files.


#JDBC Configutation for Database Connection
jdbc.driverClassName=com.mysql.jdbc.Driver
jdbc.url=jdbc:mysql://vprodb:3306/accounts?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull
jdbc.username=*****
jdbc.password=*****   <<< sensitive information

#Memcached Configuration For Active and StandBy Host
#For Active Host
memcached.active.host=vprocache01
memcached.active.port=11211
#For StandBy Host
memcached.standBy.host=vprocache02
memcached.standBy.port=11211

#RabbitMq Configuration
rabbitmq.address=vpromq01
rabbitmq.port=15672
rabbitmq.username=*****
rabbitmq.password=***** <<<< sensitive information

#Elasticesearch Configuration
elasticsearch.host =vprosearch01
elasticsearch.port =9300
elasticsearch.cluster=vprofile
elasticsearch.node=vprofilenode

## app-secret.yaml will encode the sensitive information above (rabbitmq password and mysql password) so that the passwords can be accessed by k8s.  It is simple base64 encoding but the hash provides good security 



## Clone the latest entire remote repository for this project to the kops EC2 deployment instance.

First push the latest changes above to the remote repository dmastrop/full_vprofile_app_deployment_k8s_with_kops
This has the updated volume-id of the EBS store used by the mysql pod


ubuntu@ip-172-31-88-11:~$ git clone https://github.com/dmastrop/full_vprofile_app_deployment_k8s_with_kops.git
Cloning into 'full_vprofile_app_deployment_k8s_with_kops'...
remote: Enumerating objects: 73, done.
remote: Counting objects: 100% (73/73), done.
remote: Compressing objects: 100% (55/55), done.
remote: Total 73 (delta 32), reused 57 (delta 16), pack-reused 0
Receiving objects: 100% (73/73), 17.53 KiB | 2.50 MiB/s, done.
Resolving deltas: 100% (32/32), done.
ubuntu@ip-172-31-88-11:~$ ls -la
total 72
drwxr-x--- 9 ubuntu ubuntu  4096 Apr 26 20:30 .
drwxr-xr-x 3 root   root    4096 Apr  1 23:15 ..
drwxrwxr-x 2 ubuntu ubuntu  4096 Apr  2 00:05 .aws
-rw------- 1 ubuntu ubuntu 12068 Apr 26 03:24 .bash_history
-rw-r--r-- 1 ubuntu ubuntu   220 Jan  6  2022 .bash_logout
-rw-r--r-- 1 ubuntu ubuntu  3771 Jan  6  2022 .bashrc
drwx------ 2 ubuntu ubuntu  4096 Apr  1 23:16 .cache
drwxr-xr-x 3 ubuntu ubuntu  4096 Apr 26 18:55 .kube
-rw-r--r-- 1 ubuntu ubuntu   807 Jan  6  2022 .profile
drwx------ 2 ubuntu ubuntu  4096 Apr 26 19:13 .ssh
-rw-r--r-- 1 ubuntu ubuntu     0 Apr  2 00:00 .sudo_as_admin_successful
-rw------- 1 ubuntu ubuntu   925 Apr 26 19:13 .viminfo
-rw-rw-r-- 1 ubuntu ubuntu   165 Apr  2 00:16 .wget-hsts
drwxrwxr-x 5 ubuntu ubuntu  4096 Apr 26 20:30 full_vprofile_app_deployment_k8s_with_kops
drwxrwxr-x 2 ubuntu ubuntu  4096 Apr  2 22:45 full_vprofile_app_deployment_k8s_with_kops_Backup
drwxrwxr-x 5 ubuntu ubuntu  4096 Apr 26 20:29 full_vprofile_app_deployment_k8s_with_kops_Backup2
-rw-rw-r-- 1 ubuntu ubuntu    64 Apr  2 00:07 kubectl.sha256




## create the k8s secrets with the app-secret.yaml file


ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl create -f app-secret.yaml 
secret/app-secret created
ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl get secret
NAME         TYPE     DATA   AGE
app-secret   Opaque   2      4s

ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl describe secret app-secret
Name:         app-secret
Namespace:    default
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
db-pass:   10 bytes
rmq-pass:  5 bytes


## For subsequent code changes just push the code change to the remote repo above and then do a git pull on the ubnuntu EC2 instance to pull the latest commits


## this is ready to deploy. Can first bring up db during troubleshooting and then bring up the rest, but everything can be started at once if desired


ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ ls
 ORIGINAL_k8s_yaml_from_vprofile-project_kube-app_branch   rabbitmq-deployent.yaml
 README                                                    vproapp-ClusterIP-service.yaml
 app-secret.yaml                                           vproappdep.yaml
 memcached-ClusterIP-service.yaml                         'vprodb with comments yaml'
 memcached-deployment.yaml                                 vprodb-ClusterIP-service-internal.yaml
 rabbitmq-ClusterIP-service.yaml                           vprodbdep.yaml

ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl create -f vprodbdep.yaml 
deployment.apps/vprodb created

ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl get deploy
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
vprodb   0/1     1            0           12s
ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl get deploy
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
vprodb   1/1     1            1           20s



NAME                      READY   STATUS    RESTARTS   AGE
vprodb-7cd69b5445-595md   1/1     Running   0          68s



## NOTE the volume attachment below

ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl describe pod vprodb-7cd69b5445-595md
Name:             vprodb-7cd69b5445-595md
Namespace:        default
Priority:         0
Service Account:  default
Node:             i-0ec3eab1e5bb72adb/172.20.39.94
Start Time:       Fri, 26 Apr 2024 20:46:29 +0000
Labels:           app=vprodb
                  pod-template-hash=7cd69b5445
Annotations:      kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container vprodb; cpu request for init container busybox
Status:           Running
IP:               100.96.1.6
IPs:
  IP:           100.96.1.6
Controlled By:  ReplicaSet/vprodb-7cd69b5445
Init Containers:
  busybox:
    Container ID:  containerd://3abd779c6d631599a05a542d1bf00d7fac723c11de7084b19626c5e033d03a36
    Image:         busybox:latest
    Image ID:      docker.io/library/busybox@sha256:c3839dd800b9eb7603340509769c43e146a74c63dca3045a8e7dc8ee07e53966
    Port:          <none>
    Host Port:     <none>
    Args:
      rm
      -rf
      /var/lib/mysql/lost+found
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 26 Apr 2024 20:46:36 +0000
      Finished:     Fri, 26 Apr 2024 20:46:36 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Environment:  <none>
    Mounts:
      /var/lib/mysql from vpro-db-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-w9z78 (ro)
Containers:
  vprodb:
    Container ID:   containerd://605f2c94914c229c737c8e14873331f031d558cc4fc20e16a1a6208d362c3dcf
    Image:          vprofile/vprofiledb:V1
    Image ID:       docker.io/vprofile/vprofiledb@sha256:56824afd35fabf2fafb028a90fef0e6a5cafb59519f866729be653cbba1e89cd
    Port:           3306/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 26 Apr 2024 20:46:43 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:  100m
    Environment:
      MYSQL_ROOT_PASSWORD:  <set to the key 'db-pass' in secret 'app-secret'>  Optional: false
    Mounts:
      /var/lib/mysql from vpro-db-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-w9z78 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  vpro-db-data:
    Type:       AWSElasticBlockStore (a Persistent Disk resource in AWS)   <<<<< NOTE the volumne attachment
    VolumeID:   vol-01d2206ce52103df9
    FSType:     ext4
    Partition:  0
    ReadOnly:   false
  kube-api-access-w9z78:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              zone=us-east-1a
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason                  Age   From                     Message
  ----    ------                  ----  ----                     -------
  Normal  Scheduled               84s   default-scheduler        Successfully assigned default/vprodb-7cd69b5445-595md to i-0ec3eab1e5bb72adb
  Normal  SuccessfulAttachVolume  81s   attachdetach-controller  AttachVolume.Attach succeeded for volume "ebs.csi.aws.com-vol-01d2206ce52103df9"
  Normal  Pulling                 78s   kubelet                  Pulling image "busybox:latest"
  Normal  Pulled                  77s   kubelet                  Successfully pulled image "busybox:latest" in 494.456141ms (494.46301ms including waiting)
  Normal  Created                 77s   kubelet                  Created container busybox
  Normal  Started                 77s   kubelet                  Started container busybox
  Normal  Pulling                 77s   kubelet                  Pulling image "vprofile/vprofiledb:V1"
  Normal  Pulled                  70s   kubelet                  Successfully pulled image "vprofile/vprofiledb:V1" in 6.872954992s (6.873016765s including waiting)
  Normal  Created                 70s   kubelet                  Created container vprodb
  Normal  Started                 70s   kubelet                  Started container vprodb


## Bring up the ClusterIP for the db

ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl create -f vprodb-ClusterIP-service-internal.yaml 
service/vprodb created
ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl get service
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   100.64.0.1       <none>        443/TCP    112m
vprodb       ClusterIP   100.65.138.179   <none>        3306/TCP   6s


## bring up the rest of the stack (disregard errors below; those resources already started/created)


ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl create -f .
service/vprocache01 created
deployment.apps/vpromc created
service/vpromq01 created
deployment.apps/vpromq01 created
service/vproapp-service created
deployment.apps/vproapp created
Error from server (AlreadyExists): error when creating "app-secret.yaml": secrets "app-secret" already exists
Error from server (AlreadyExists): error when creating "vprodb-ClusterIP-service-internal.yaml": services "vprodb" already exists
Error from server (AlreadyExists): error when creating "vprodbdep.yaml": deployments.apps "vprodb" already exists



## kubectl get the status

ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl get service
NAME              TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)        AGE
kubernetes        ClusterIP      100.64.0.1       <none>                                                                   443/TCP        119m
vproapp-service   LoadBalancer   100.68.104.188   aa89387a9002c4068a3b8760147091ba-283941963.us-east-1.elb.amazonaws.com   80:30902/TCP   74s
vprocache01       ClusterIP      100.66.4.41      <none>                                                                   11211/TCP      74s
vprodb            ClusterIP      100.65.138.179   <none>                                                                   3306/TCP       7m26s
vpromq01          ClusterIP      100.68.145.165   <none>                                                                   15672/TCP      74s


ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
vproapp-5d64cd6976-8stj7    1/1     Running   0          79s
vprodb-7cd69b5445-595md     1/1     Running   0          12m
vpromc-5c65464866-pmvkt     1/1     Running   0          79s
vpromq01-69b8ff77dc-m9fm6   1/1     Running   0          79s


ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl get deploy
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
vproapp    1/1     1            1           91s
vprodb     1/1     1            1           12m
vpromc     1/1     1            1           91s
vpromq01   1/1     1            1           91s



## An Elastic Load Balancer is created for the front end. It is not nginix so it will not show up in the pods under the ingress-nginx namespace

The loadbalancer can be seen with the kubectl describe service vproapp-service


ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl describe service vproapp-service
Name:                     vproapp-service
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=vproapp
Type:                     LoadBalancer
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       100.68.104.188
IPs:                      100.68.104.188
LoadBalancer Ingress:     aa89387a9002c4068a3b8760147091ba-283941963.us-east-1.elb.amazonaws.com
Port:                     <unset>  80/TCP
TargetPort:               vproapp-port/TCP
NodePort:                 <unset>  30902/TCP
Endpoints:                100.96.1.48:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:
  Type    Reason                Age   From                Message
  ----    ------                ----  ----                -------
  Normal  EnsuringLoadBalancer  21m   service-controller  Ensuring load balancer
  Normal  EnsuredLoadBalancer   21m   service-controller  Ensured load balancer



 ## Create an A record in Route53 under the hosted zone for this cluster. Use the DNS name of the loadbalancer.

Route 53 ---> create a Record. Switch to wizard
Choose simple routing (this is what nginx would normally do)
Define simple record
XXXXXXX.holinessinloveofchrist.com
Use A record
Alias to Application and Classic LoadBalancer
Choose region us-east-1
Choose the loadbalancer that was created from the pulldown
Create Record


This will create the DNS for the frontend loadbalancer above.



## If get an error in loadbalancer creation this could be because the proper service role has not been created in the AWS account yet. 

Run this in the kops EC2 instance

aws iam create-service-linked-role --aws-service-name "elasticloadbalancing.amazonaws.com"

This will create the service role.



## The browser should function with the full stack application functionality

xxxxxxx.holinessinloveofchrist.com





## commands and cluster DELETE process

kubectl apply OR

Kubectl create -f <>
Kubectl create -f .
Kubectl log <pod_name>
Kubectl get pods
Kubectl describe pod <>
Kubectl get service
Kubectl describe service <>

Kubectl delete -f .

This deletes the ALB loadbalancer but does not delete the db volume. That gets deleted when run kops delete cluster

Note if delete directly with kops it will delete the ALB etc as well.
Note that the attached volume to the db will be deleted as well.

After kubectl delete -f . do a kops delete cluster
kops delete cluster --name=kops-project14.holinessinloveofchrist.com --state=s3:/vprofile-kops-s3-state-project14 --yes



Shut down the EC2 instance.

Does the EBS volume get removed as well?  Yes the volume does get removed. Have to redo it each time from scratch.



========

ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl delete -f .
secret "app-secret" deleted
service "vprocache01" deleted
deployment.apps "vpromc" deleted
service "vpromq01" deleted
deployment.apps "vpromq01" deleted
service "vproapp-service" deleted
deployment.apps "vproapp" deleted
service "vprodb" deleted
deployment.apps "vprodb" deleted


ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl get service
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   100.64.0.1   <none>        443/TCP   155m
ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl get secret
No resources found in default namespace.
ubuntu@ip-172-31-88-11:~/full_vprofile_app_deployment_k8s_with_kops$ kubectl get pod
No resources found in default namespace.


At this point the loadbalancer is deleted





