# Summary of project 14


Project 14. This is a full stack deployment of the vprofile java app ( frontend tomcat, memcached, rabbitmq and   backend mysql) onto a k8s cluster created with kops. This is meant to be a production grade roll out of the   **containerized** version of the vprofile java app onto k8s. Use EC2 instance on AWS2 with kubectl, aws cli,kops   
installed/IAM  


# Detail on the implementation:

## First get the k8s yaml files from the vprofile-project (branch kube-app branch)

## Getting kops up using the EC2 instance kops-project14-EC2

The EC2 instance has to have aws cli and kops installed as well as kubectl.  
ubuntu@ip-172-31-88-11:~$ kops version  
Client version: 1.26.4 (git-v1.26.4)  
ubuntu@ip-172-31-88-11:~$ aws --version  
aws-cli/1.22.34 Python/3.10.12 Linux/6.5.0-1016-aws botocore/1.23.34  
ubuntu@ip-172-31-88-11:~$ kubectl version --client  
Client Version: v1.29.3  
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3  

The EC2 instance has the kops ssh key 

## AWS access key and secret key are already set on the EC2 instance for the apppropriate k8s cluster permissions 

aws configure list will show the IAM user already configured.

## test the Route53 hosted zone that is designated for the k8s cluster creation with kops

dig NS kops-project14.xxxxx.com
nslookup -type=ns kops-project14.xxxxx.com

The NS should show the AWS name servers for the above.


## The EC2 instance is kops-project14-EC2

The SSH key for the instance is kops-project14-rsa-pem.pem
ssh -i kops-project14-rsa-pem.pem ubuntu@<EC2_instance_public_ip>

The EC2 instance has an rsa generated keypair (kops-key-project14 and .pub) to be used with the kops configuration and deployment so that
the control master and worker nodes can be accessed from EC2 to the cluster.


% ssh -i kops-project14-rsa-pem.pem ubuntu@44.204.236.93 

Last login: Fri Apr 26 02:44:55 2024 from 73.202.248.175
ubuntu@ip-172-31-88-11:~$ 

ubuntu@ip-172-31-88-11:~$ ls ~/.ssh
authorized_keys  known_hosts  known_hosts.old  kops-key-project14  kops-key-project14.pub


## Configure the cluster with kops

Note that the S3 bucket has to be preconfigured

kops create cluster --name=kops-project14.holinessinloveofchrist.com \
--state=s3://vprofile-kops-s3-state-project14 --zones=us-east-1a,us-east-1b \
--node-count=2 --node-size=t3.small --master-size=t3.medium --dns-zone=kops-project14.holinessinloveofchrist.com \
--node-volume-size=8 --master-volume-size=8 --ssh-public-key=~/.ssh/kops-key-project14.pub


## Deploy the cluster with the above configuration

kops update cluster --name=kops-project14.holinessinloveofchrist.com --state=s3://vprofile-kops-s3-state-project14 --yes --admin

## Validate the cluster (node state)

kops validate cluster --state=s3://vprofile-kops-s3-state-project14


ubuntu@ip-172-31-88-11:~$ kops validate cluster --state=s3://vprofile-kops-s3-state-project14
Using cluster from kubectl context: kops-project14.holinessinloveofchrist.com

Validating cluster kops-project14.holinessinloveofchrist.com

INSTANCE GROUPS
NAME                            ROLE            MACHINETYPE     MIN     MAX     SUBNETS
control-plane-us-east-1a        ControlPlane    t3.medium       1       1       us-east-1a
nodes-us-east-1a                Node            t3.small        1       1       us-east-1a
nodes-us-east-1b                Node            t3.small        1       1       us-east-1b

NODE STATUS
NAME                    ROLE            READY
i-002c39301f5dc9bb2     node            True
i-050ff1a6e0e0fe532     control-plane   True
i-0ec3eab1e5bb72adb     node            True


ubuntu@ip-172-31-88-11:~$ kubectl get node
NAME                  STATUS   ROLES           AGE     VERSION
i-002c39301f5dc9bb2   Ready    node            7m34s   v1.26.13
i-050ff1a6e0e0fe532   Ready    control-plane   10m     v1.26.13
i-0ec3eab1e5bb72adb   Ready    node            7m43s   v1.26.13


## optionally can ssh into the master control node if required (note clean up the ~/.ssh//known_hosts file with older keys may be required)

ubuntu@ip-172-31-88-11:~$ ssh -i ~/.ssh/kops-key-project14 ubuntu@api.kops-project14.holinessinloveofchrist.com
The authenticity of host 'api.kops-project14.holinessinloveofchrist.com (44.203.96.195)' can't be established.
ED25519 key fingerprint is SHA256:bpckAK8QhWOGNmd5l4+r396Ye5BpJpLSvmR6/7aIVBk.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added 'api.kops-project14.holinessinloveofchrist.com' (ED25519) to the list of known hosts.
Welcome to Ubuntu 20.04.6 LTS (GNU/Linux 5.15.0-1052-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

  System information as of Fri Apr 26 19:13:12 UTC 2024

  System load:  0.93              Processes:                    160
  Usage of /:   79.9% of 7.57GB   Users logged in:              0
  Memory usage: 34%               IPv4 address for cilium_host: 100.96.0.207
  Swap usage:   0%                IPv4 address for ens5:        172.20.48.241

Expanded Security Maintenance for Applications is not enabled.

60 updates can be applied immediately.
49 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable

Enable ESM Apps to receive additional future security updates.
See https://ubuntu.com/esm or run: sudo pro status



The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

To run a command as administrator (user "root"), use "sudo <command>".
See "man sudo_root" for details.

ubuntu@i-050ff1a6e0e0fe532:~$ 





## Once the cluster is up, the setup is ready to apply the k8s yaml configuration files for the full vprofile stack (application and backend server infrastructure)

To get the full stack up this project uses public docker images for memcached and rabbitmq
The database container is a customized docker image vprofile/vprofiledb:V1
Likewise the vprofileapp image is a customized docker image as well vprofile/vprofileapp:V1
This  image was created from the source code on the vprofile-project docker branch (must use the docker branch)
More on this in a section below.
The application.properties endpoint file access is configured using local k8s hostnames as the entire server infra is on k8s network fabric and is flat. So complex AWS endpoints are not required.
All the sensitive login information is in the appplication.properties file, but this will be built into the container and never exposed
The private network is flat and permits routing server to server by default.   This is a major deployment advantage of deploying containers to k8s cluster for orchestration rather than deploying with EC2 instances.  No explicit security groups need to be configured in this k8s deployment of the full stack, unlike with an EC2 instance deployment and no complex endpoint URLs need to be configured in the application.properties file.





## One prerequisite prior to applying the yaml files to the cluster is to configure an EBS (Elastic Block store) standard volume in the same region as the cluster (us-east-1) so that the mysql db containter can store /var/lib/mysql into the EBS volume. The db container itself has no provision for this external volume.  In addtion the volume must be in the same availablity zone as the node that has the db pod.  (See further below on how to ensure this)

This can be done from the VSCode terminal.
Make sure to configure the terminal with the AWS2 account (not the AWS1 account). Use the admin user IAMidadave
export AWS_PROFILE=IAMidadave
% echo $AWS_PROFILE
IAMidadave

aws configure list to make sure the correct user is used. This is an Admin user.
aws ec2 create-volume --availability-zone=us-east-1a  --region=us-east-1 --size=3 --volume-type=gp2

aws ec2 create-volume --availability-zone=us-east-1a  --region=us-east-1 --size=3 --volume-type=gp2
{
    "AvailabilityZone": "us-east-1a",  <<<< must be in this availablity zone for reasons below
    "CreateTime": "2024-04-26T19:39:43+00:00",
    "Encrypted": false,
    "Size": 3,
    "SnapshotId": "",
    "State": "creating",
    "VolumeId": "vol-01d2206ce52103df9",   <<< volume id
    "Iops": 100,
    "Tags": [],
    "VolumeType": "gp2",
    "MultiAttachEnabled": false
}
:


## NOTE: the pod running the mysql db must be running on a node in the SAME region AND same availability zone as the EBS created above. To do this use the following code in the mysql db deployment yaml  (the nodes must also be explicitly labeled with the availability zones; see next section below)

      nodeSelector:
        zone: us-east-1a
        # we created labels for the 2 nodes in the kops cluser: us-east-1a and us-east-1b
        # This nodeSelector will ensure that this db container runs on this us-east-1a node
        # this will ensure that the db container is in the same zone as the volume block store that it is using
        # The volume block store was manually created in us-east-1a



## This is how to label the  nodes in the cluster with the availability zone that they are in. This must be done for the reasons above, to correlate node availability zone to the EBS availability zone.

ubuntu@ip-172-31-88-11:~$ kubectl get nodes --show-labels
NAME                  STATUS   ROLES           AGE   VERSION    LABELS
i-002c39301f5dc9bb2   Ready    node            43m   v1.26.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t3.small,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1b,kubernetes.io/arch=amd64,kubernetes.io/hostname=i-002c39301f5dc9bb2,kubernetes.io/os=linux,node-role.kubernetes.io/node=,node.kubernetes.io/instance-type=t3.small,topology.ebs.csi.aws.com/zone=us-east-1b,topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1b

i-050ff1a6e0e0fe532   Ready    control-plane   46m   v1.26.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t3.medium,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1a,kops.k8s.io/kops-controller-pki=,kubernetes.io/arch=amd64,kubernetes.io/hostname=i-050ff1a6e0e0fe532,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=,node.kubernetes.io/instance-type=t3.medium,topology.ebs.csi.aws.com/zone=us-east-1a,topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1a

i-0ec3eab1e5bb72adb   Ready    node            43m   v1.26.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t3.small,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1a,kubernetes.io/arch=amd64,kubernetes.io/hostname=i-0ec3eab1e5bb72adb,kubernetes.io/os=linux,node-role.kubernetes.io/node=,node.kubernetes.io/instance-type=t3.small,topology.ebs.csi.aws.com/zone=us-east-1a,topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1a

the last node in the output above i-0ec3eab1e5bb72adb  is the worker node where the mysql db has to be run since that is in the same availability zone as the EBS store created above

ubuntu@ip-172-31-88-11:~$ kubectl describe node i-0ec3eab1e5bb72adb |grep us-east-1
                    failure-domain.beta.kubernetes.io/region=us-east-1
                    failure-domain.beta.kubernetes.io/zone=us-east-1a
                    topology.ebs.csi.aws.com/zone=us-east-1a
                    topology.kubernetes.io/region=us-east-1
                    topology.kubernetes.io/zone=us-east-1a
ProviderID:                   aws:///us-east-1a/i-0ec3eab1e5bb72adb




## label the nodes with availabilty zone


ubuntu@ip-172-31-88-11:~$ kubectl label nodes i-0ec3eab1e5bb72adb zone=us-east-1a
node/i-0ec3eab1e5bb72adb labeled
ubuntu@ip-172-31-88-11:~$ kubectl label nodes i-002c39301f5dc9bb2 zone=us-east-1b
node/i-002c39301f5dc9bb2 labeled


## the worker nodes are now labeled:

ubuntu@ip-172-31-88-11:~$ kubectl get nodes --show-labels
NAME                  STATUS   ROLES           AGE   VERSION    LABELS
i-002c39301f5dc9bb2   Ready    node            50m   v1.26.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t3.small,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1b,kubernetes.io/arch=amd64,kubernetes.io/hostname=i-002c39301f5dc9bb2,kubernetes.io/os=linux,node-role.kubernetes.io/node=,node.kubernetes.io/instance-type=t3.small,topology.ebs.csi.aws.com/zone=us-east-1b,topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1b,zone=us-east-1b  <<<<<<<<<

i-050ff1a6e0e0fe532   Ready    control-plane   53m   v1.26.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t3.medium,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1a,kops.k8s.io/kops-controller-pki=,kubernetes.io/arch=amd64,kubernetes.io/hostname=i-050ff1a6e0e0fe532,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=,node.kubernetes.io/instance-type=t3.medium,topology.ebs.csi.aws.com/zone=us-east-1a,topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1a


i-0ec3eab1e5bb72adb   Ready    node            50m   v1.26.13   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t3.small,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1a,kubernetes.io/arch=amd64,kubernetes.io/hostname=i-0ec3eab1e5bb72adb,kubernetes.io/os=linux,node-role.kubernetes.io/node=,node.kubernetes.io/instance-type=t3.small,topology.ebs.csi.aws.com/zone=us-east-1a,topology.kubernetes.io/region=us-east-1,topology.kubernetes.io/zone=us-east-1a,zone=us-east-1a   <<<<<<<<<<


## at this point the EBS volume is in availablity zone us-east-1a and the nodes are labeled with avaialbity zone us-east-1a and the nodeSelector of the mysql db pod deployment yaml is set to us-east-1a.  




## yaml code (partial excerpt) for the volumeMount of the volume in the next section below. This is in the db deployment yaml file.

spec:
      containers:
        - name: vprodb
          image: vprofile/vprofiledb:V1
          # use the image on docker hub vprofile/vprofiledb:V1
            
          # USE an initContainer (bottom, below) instead for the removal of the lost+found directory on the volumne
          # args:
          #   - "--ignore-db-dir=lost+found"

          # map the volume defined at bottom below to this db container pod
          volumeMounts:
            - mountPath: /var/lib/mysql
            # the volume will be mounted to this directory path on the db container
              name: vpro-db-data
              # name is below

## yaml code for the database deployment yaml file. Insert this at the bottom of the file.  This volumeID has be be manually added, for now,  to the yaml after creating the volume on EBS.  Note it has to be tagged on AWS accordingly with cluster name.  The initContainer block is just to remove the lost+found folder on the default volume file structure. This will cause problems when the db container tries to do the mountVolume above. Removing it is the best approach.

     volumes:
      # attach this volume to this db container defined above
        - name: vpro-db-data
          awsElasticBlockStore:
            volumeID: vol-01d2206ce52103df9  <<<<<<<<<<<<<< update this with the new EBS volumne created above
            # the volume id is from AWS cli when we created the volume
            # this is vol-0b308063deec65e5e
            fsType: ext4
            # to attach to the volume in k8s we need to tag the AWS volume
            # We can do this through aws cli or through the web console.
            # the tag must use ClusterName as key and the name of the kops clustser as the value
            # Without the tag will get a permissions denied error
            # Map the volume to the pod above wiht the volumenMounts (see above)
            # Note with ext4 fsType there will be a lost+found diretory created by linux
            # this will be a problem when the db container tries to mount this volume and map it.
            # Must either ignore the folder or before mysql starts delete the folder
            # To ignore pass args tp the container.(see above)  This only works with mysql 5 not with 8
            # Better to do an init container that starts on the pod prior to the db container. This will remove the folder prior to the db container starting
            # on the pod.
      initContainers:
      # once the initContainer removes the folder below it will exit and the db container will come up and
      # mount the same volume, but with no lost+found folder
        - name: busybox
          # name can be anything
          image: busybox:latest
          args: ["rm", "-rf", "/var/lib/mysql/lost+found"]
          # this arg will remove the folder lost+found from the mounted directory 
          # we will mount the volume to this  lightweight container to perform the deletion
          volumeMounts:
            - name: vpro-db-data
              mountPath: /var/lib/mysql
              # the volume will be mounted to this directory path on the db container




## Tag the db EBS volume created above with the tag KubernetesCluster and the value of name of the kops cluster which is kops-project14.holinessinloveofchrist.com (the hosted zone name on Route53 and kops cluster name)

This is done through the AWS web console


## the vprofile/vprofileapp:V1 docker image used for the vprofile app frontend is an elastic loadbalancer rather than an nginx container.  The docker image was created using source in vprofile-project and using docker branch

The frontend app artifact .war file is embedded in the tomcat container instance as ROOT.war in the home directory of the tomcat server

FROM tomcat:8-jre11
LABEL "Project"="Vprofile"
LABEL "Author"="xxxxxxx"

RUN rm -rf /usr/local/tomcat/webapps/*
COPY target/vprofile-v2.war /usr/local/tomcat/webapps/ROOT.war

EXPOSE 8080
CMD ["catalina.sh", "run"]
WORKDIR /usr/local/tomcat/
VOLUME /usr/local/tomcat/webapps





## The application.properties file embedded in the built docker container image is below.  The sensitive information is **** out.  Note the "endpoints" are simply hostnames that will be configured in the corresponding SERVICE yaml files for each of the servers (pods) below.  This is a major advantage to deloying more complex full stack applications that involve multiple server and backend servers. Similarly the ports will be defined in the corresponding SERVICE yaml files for the applications/servers below.


Note the port numbers. These will be required for the yaml k8s definition files.


#JDBC Configutation for Database Connection
jdbc.driverClassName=com.mysql.jdbc.Driver
jdbc.url=jdbc:mysql://vprodb:3306/accounts?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull
jdbc.username=*****
jdbc.password=*****   <<< sensitive information

#Memcached Configuration For Active and StandBy Host
#For Active Host
memcached.active.host=vprocache01
memcached.active.port=11211
#For StandBy Host
memcached.standBy.host=vprocache02
memcached.standBy.port=11211

#RabbitMq Configuration
rabbitmq.address=vpromq01
rabbitmq.port=15672
rabbitmq.username=*****
rabbitmq.password=***** <<<< sensitive information

#Elasticesearch Configuration
elasticsearch.host =vprosearch01
elasticsearch.port =9300
elasticsearch.cluster=vprofile
elasticsearch.node=vprofilenode

## app-secret.yaml will encode the sensitive information above (rabbitmq password and mysql password) so that the passwords can be accessed by k8s.  It is simple base64 encoding but the hash provides good security 



## Clone the latest entire remote repository for this project to the kops EC2 deployment instance.

First push the latest changes above to the remote repository dmastrop/full_vprofile_app_deployment_k8s_with_kops



